{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6a872b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Resumes:\n",
      "1. 2 Pager Resume.pdf (Cosine Similarity: 0.3455)\n",
      "2. my resume.pdf (Cosine Similarity: 0.3142)\n",
      "3. Manish Chaudhary CV.pdf (Cosine Similarity: 0.1966)\n",
      "4. SouravResume  - 0-2 (2).pdf (Cosine Similarity: 0.1625)\n",
      "5. SOUVICK GHOSH.pdf (Cosine Similarity: 0.1166)\n"
     ]
    }
   ],
   "source": [
    "# Model 1 point = 1\n",
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2 as pdf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        pdf_reader = pdf.PdfReader(pdf_path)\n",
    "        return pdf_reader.pages[0].extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_cosine_similarity(given_text, pdf_text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([given_text, pdf_text])\n",
    "    return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "def main(folder_path, given_text):\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    # Initialize a dictionary to store cosine similarity scores\n",
    "    similarity_scores = {}\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        similarity = calculate_cosine_similarity(given_text, pdf_text)\n",
    "        similarity_scores[pdf_file] = similarity\n",
    "\n",
    "    # Sort the scores in descending order\n",
    "    sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the top 20 matches\n",
    "    print(\"Top 20 Resumes:\")\n",
    "    for i, (resume, score) in enumerate(sorted_scores[:20], start=1):\n",
    "        print(f\"{i}. {resume} (Cosine Similarity: {score:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"C:/Users/chakr/OneDrive/Desktop/resumes\"\n",
    "    given_text =  \"\"\"As a Data Scientist at Zyient.io, you will play a pivotal role in driving data-driven solutions within our Application Team. Leveraging your expertise in AI and ML, you will work on developing and implementing advanced models and algorithms to extract valuable insights from data, enhance OCR capabilities, and revolutionize processes in the Insurance and Accounts Receivable sectors.\n",
    "\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "You’ll be working on AI platforms, which are a set of products responsible for building, training & deploying ML models for extraction & interpretation of semi & unstructured documents using NLP & Computer vision.\n",
    "You’ll be working on ML model development as well as building automated training pipelines that deliver high-performance models on a given dataset.\n",
    "You’ll have to work with application team to understand their data extraction requirements that can be solved using ML.\n",
    "Educate SMEs on ensuring high-quality data annotations & help them to validate your ML models.\n",
    "Take part in building APIs around your model for production & ensure that it is able to deliver expected accuracies & throughput requirements.\n",
    "\n",
    "\n",
    "Skillset: \n",
    "\n",
    "\n",
    "\n",
    "Must have Minimum 2 (for DS-1) and 4 (for DS-2) years of Data Science Experience\n",
    "Strong theoretical & practical knowledge of ML model development, hyper-parameter tuning & production deployment.\n",
    "Strong experience in building models using libraries like Tensorflow, Pytorch\n",
    "Good experience in writing code in Python (3.x)\n",
    "Understanding of well-known architecture/algorithms in NLP like Transformers, LSTMs & GRUs\n",
    "Experience in fine-tuning pre-trained models like BERT, and ELECTRA for downstream tasks such as NER, Classification, etc.\n",
    "Understanding of Object detection using libraries like Yolo or TF object detection API\n",
    "Knowledge of standard packaging & deployment solutions like Tensorflow Model Server, MLflow, or ONNX.\n",
    "Practical knowledge of libraries like Numpy, Pandas & Spacy\n",
    "Practical knowledge of building RESTful APIs around your model using Fast/Flask API\n",
    "Strong Understanding on MLOPs - Life cycle of Machine Learning Models\n",
    "\"\"\"\n",
    "    main(folder_path, given_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfa9402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chakr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Resumes:\n",
      "1. my resume.pdf (Cosine Similarity: 0.1583)\n",
      "2. 2 Pager Resume.pdf (Cosine Similarity: 0.1286)\n",
      "3. Manish Chaudhary CV.pdf (Cosine Similarity: 0.0442)\n",
      "4. SouravResume  - 0-2 (2).pdf (Cosine Similarity: 0.0436)\n",
      "5. SOUVICK GHOSH.pdf (Cosine Similarity: 0.0327)\n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2 as pdf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stop words (if not already downloaded)\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        pdf_reader = pdf.PdfReader(pdf_path)\n",
    "        return pdf_reader.pages[0].extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def calculate_cosine_similarity(given_text, pdf_text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Remove stop words from both texts\n",
    "    given_text_cleaned = \" \".join([word for word in given_text.lower().split() if word not in stop_words])\n",
    "    pdf_text_cleaned = \" \".join([word for word in pdf_text.lower().split() if word not in stop_words])\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([given_text_cleaned, pdf_text_cleaned])\n",
    "    return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "def main(folder_path, given_text):\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    # Initialize a dictionary to store cosine similarity scores\n",
    "    similarity_scores = {}\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        similarity = calculate_cosine_similarity(given_text, pdf_text)\n",
    "        similarity_scores[pdf_file] = similarity\n",
    "\n",
    "    # Sort the scores in descending order\n",
    "    sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the top 20 matches\n",
    "    print(\"Top 20 Resumes:\")\n",
    "    for i, (resume, score) in enumerate(sorted_scores[:20], start=1):\n",
    "        print(f\"{i}. {resume} (Cosine Similarity: {score:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"C:/Users/chakr/OneDrive/Desktop/resumes\"\n",
    "    given_text = \"\"\"As a Data Scientist at Zyient.io, you will play a pivotal role in driving data-driven solutions within our Application Team. Leveraging your expertise in AI and ML, you will work on developing and implementing advanced models and algorithms to extract valuable insights from data, enhance OCR capabilities, and revolutionize processes in the Insurance and Accounts Receivable sectors.\n",
    "\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "You’ll be working on AI platforms, which are a set of products responsible for building, training & deploying ML models for extraction & interpretation of semi & unstructured documents using NLP & Computer vision.\n",
    "You’ll be working on ML model development as well as building automated training pipelines that deliver high-performance models on a given dataset.\n",
    "You’ll have to work with application team to understand their data extraction requirements that can be solved using ML.\n",
    "Educate SMEs on ensuring high-quality data annotations & help them to validate your ML models.\n",
    "Take part in building APIs around your model for production & ensure that it is able to deliver expected accuracies & throughput requirements.\n",
    "\n",
    "\n",
    "Skillset: \n",
    "\n",
    "\n",
    "\n",
    "Must have Minimum 2 (for DS-1) and 4 (for DS-2) years of Data Science Experience\n",
    "Strong theoretical & practical knowledge of ML model development, hyper-parameter tuning & production deployment.\n",
    "Strong experience in building models using libraries like Tensorflow, Pytorch\n",
    "Good experience in writing code in Python (3.x)\n",
    "Understanding of well-known architecture/algorithms in NLP like Transformers, LSTMs & GRUs\n",
    "Experience in fine-tuning pre-trained models like BERT, and ELECTRA for downstream tasks such as NER, Classification, etc.\n",
    "Understanding of Object detection using libraries like Yolo or TF object detection API\n",
    "Knowledge of standard packaging & deployment solutions like Tensorflow Model Server, MLflow, or ONNX.\n",
    "Practical knowledge of libraries like Numpy, Pandas & Spacy\n",
    "Practical knowledge of building RESTful APIs around your model using Fast/Flask API\n",
    "Strong Understanding on MLOPs - Life cycle of Machine Learning Models\n",
    "\"\"\"\n",
    "    main(folder_path, given_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "991ba7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chakr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Resumes:\n",
      "1. my resume.pdf (Cosine Similarity: 0.2061)\n",
      "2. 2 Pager Resume.pdf (Cosine Similarity: 0.1711)\n",
      "3. SouravResume  - 0-2 (2).pdf (Cosine Similarity: 0.0876)\n",
      "4. Manish Chaudhary CV.pdf (Cosine Similarity: 0.0661)\n",
      "5. SOUVICK GHOSH.pdf (Cosine Similarity: 0.0489)\n"
     ]
    }
   ],
   "source": [
    "# Model 3\n",
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2 as pdf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stop words (if not already downloaded)\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        pdf_reader = pdf.PdfReader(pdf_path)\n",
    "        return pdf_reader.pages[0].extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Remove stop words and perform stemming\n",
    "    cleaned_tokens = [stemmer.stem(word) for word in text.lower().split() if word not in stop_words]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "def calculate_cosine_similarity(given_text, pdf_text):\n",
    "    given_text_cleaned = preprocess_text(given_text)\n",
    "    pdf_text_cleaned = preprocess_text(pdf_text)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([given_text_cleaned, pdf_text_cleaned])\n",
    "    return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "def main(folder_path, given_text):\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    similarity_scores = {}\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        similarity = calculate_cosine_similarity(given_text, pdf_text)\n",
    "        similarity_scores[pdf_file] = similarity\n",
    "\n",
    "    sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Top 20 Resumes:\")\n",
    "    for i, (resume, score) in enumerate(sorted_scores[:20], start=1):\n",
    "        print(f\"{i}. {resume} (Cosine Similarity: {score:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"C:/Users/chakr/OneDrive/Desktop/resumes\"\n",
    "    given_text = \"\"\"\n",
    "  As a Data Scientist at Zyient.io, you will play a pivotal role in driving data-driven solutions within our Application Team. Leveraging your expertise in AI and ML, you will work on developing and implementing advanced models and algorithms to extract valuable insights from data, enhance OCR capabilities, and revolutionize processes in the Insurance and Accounts Receivable sectors.\n",
    "\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "You’ll be working on AI platforms, which are a set of products responsible for building, training & deploying ML models for extraction & interpretation of semi & unstructured documents using NLP & Computer vision.\n",
    "You’ll be working on ML model development as well as building automated training pipelines that deliver high-performance models on a given dataset.\n",
    "You’ll have to work with application team to understand their data extraction requirements that can be solved using ML.\n",
    "Educate SMEs on ensuring high-quality data annotations & help them to validate your ML models.\n",
    "Take part in building APIs around your model for production & ensure that it is able to deliver expected accuracies & throughput requirements.\n",
    "\n",
    "\n",
    "Skillset: \n",
    "\n",
    "\n",
    "\n",
    "Must have Minimum 2 (for DS-1) and 4 (for DS-2) years of Data Science Experience\n",
    "Strong theoretical & practical knowledge of ML model development, hyper-parameter tuning & production deployment.\n",
    "Strong experience in building models using libraries like Tensorflow, Pytorch\n",
    "Good experience in writing code in Python (3.x)\n",
    "Understanding of well-known architecture/algorithms in NLP like Transformers, LSTMs & GRUs\n",
    "Experience in fine-tuning pre-trained models like BERT, and ELECTRA for downstream tasks such as NER, Classification, etc.\n",
    "Understanding of Object detection using libraries like Yolo or TF object detection API\n",
    "Knowledge of standard packaging & deployment solutions like Tensorflow Model Server, MLflow, or ONNX.\n",
    "Practical knowledge of libraries like Numpy, Pandas & Spacy\n",
    "Practical knowledge of building RESTful APIs around your model using Fast/Flask API\n",
    "Strong Understanding on MLOPs - Life cycle of Machine Learning Models\n",
    "\n",
    "\"\"\"\n",
    "    main(folder_path, given_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ff447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b59d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
